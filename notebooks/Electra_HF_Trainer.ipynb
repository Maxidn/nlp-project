{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGLpbq-vLhez"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ INSTALL DEPENDENCIES\n",
        "!pip install evaluate\n",
        "!pip install optuna\n",
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ IMPORTS\n",
        "import os\n",
        "import re\n",
        "import wandb\n",
        "import optuna\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification, AutoConfig,\n",
        "    TrainingArguments, Trainer, EarlyStoppingCallback,\n",
        "    default_data_collator\n",
        ")\n",
        "from transformers.integrations import WandbCallback\n",
        "from datasets import Dataset\n",
        "from datetime import datetime\n",
        "\n",
        "from optuna import trial\n"
      ],
      "metadata": {
        "id": "C9Uz1USzLnsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ MOUNT DRIVE\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VKWJNb_FLqN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "5ZWKo4NLLt8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ CONSTANTS\n",
        "MODEL_NAME_TWITTER = \"google/electra-base-discriminator\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "# Optional: safer SDPA fallback on some Colab combos\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True)\n"
      ],
      "metadata": {
        "id": "FIG79KxhLx0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ LOAD & CLEAN DATA\n",
        "label2id = {\n",
        "    \"Extremely Negative\": 0,\n",
        "    \"Negative\": 1,\n",
        "    \"Neutral\": 2,\n",
        "    \"Positive\": 3,\n",
        "    \"Extremely Positive\": 4\n",
        "}\n",
        "\n",
        "train_path = \"/content/drive/MyDrive/◊™◊ï◊ê◊® ◊©◊†◊ô/deep/Corona_NLP_train_clean.xls\" #Change location where you saved the cleaned data\n",
        "test_path = \"/content/drive/MyDrive/◊™◊ï◊ê◊® ◊©◊†◊ô/deep/Corona_NLP_test_clean.xls\" #Change location where you saved the cleaned data\n",
        "\n",
        "train_df = pd.read_csv(train_path, encoding=\"ISO-8859-1\")\n",
        "test_df  = pd.read_csv(test_path,  encoding=\"ISO-8859-1\")\n",
        "\n",
        "train_df[\"label\"] = train_df[\"Sentiment\"].map(label2id)\n",
        "test_df[\"label\"]  = test_df[\"Sentiment\"].map(label2id)\n",
        "\n",
        "train_df = train_df[[\"cleaned_tweets\",\"label\"]].dropna(subset=[\"cleaned_tweets\"])\n",
        "test_df  = test_df[[\"cleaned_tweets\",\"label\"]].dropna(subset=[\"cleaned_tweets\"])\n",
        "\n",
        "def normalize_tweet(t: str) -> str:\n",
        "    t = str(t)\n",
        "    t = re.sub(r\"http\\S+\", \"<url>\", t)\n",
        "    t = re.sub(r\"@\\w+\", \"<user>\", t)\n",
        "    t = re.sub(r\"\\d+\", \"<number>\", t)\n",
        "    return t.strip()\n",
        "\n",
        "for df in (train_df, test_df):\n",
        "    df[\"cleaned_tweets\"] = df[\"cleaned_tweets\"].astype(str).map(normalize_tweet)\n",
        "    df.query(\"cleaned_tweets.str.len() >= 3\", engine=\"python\", inplace=True)\n",
        "\n",
        "train_df[\"label\"] = train_df[\"label\"].astype(int)\n",
        "test_df[\"label\"]  = test_df[\"label\"].astype(int)\n",
        "\n",
        "# Split\n",
        "train_df, val_df = train_test_split(\n",
        "    train_df, test_size=0.2, stratify=train_df[\"label\"], random_state=42\n",
        ")\n",
        "test_df_final = test_df.copy()\n",
        "\n",
        "# Hard checks\n",
        "assert {\"cleaned_tweets\",\"label\"}.issubset(train_df.columns)\n",
        "assert train_df[\"label\"].between(0,4).all() and val_df[\"label\"].between(0,4).all()\n"
      ],
      "metadata": {
        "id": "BkyVICKNL0sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ TOKENIZER + SPECIAL TOKENS (ADD ONCE, OUTSIDE OBJECTIVE)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_TWITTER, use_fast=True)\n",
        "\n",
        "# Ensure pad token (usually present for RoBERTa; safe to check)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
        "\n",
        "# Add your custom specials only if missing\n",
        "custom_specials = [\"<url>\", \"<user>\", \"<number>\"]\n",
        "to_add = [t for t in custom_specials if t not in tokenizer.get_vocab()]\n",
        "num_added = 0\n",
        "if to_add:\n",
        "    num_added = tokenizer.add_special_tokens({\"additional_special_tokens\": to_add})\n",
        "\n",
        "print(f\"Added {num_added} new tokens: {to_add if to_add else '[]'}\")"
      ],
      "metadata": {
        "id": "YG-yau_EAZEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ DATASETS\n",
        "train_dataset_hf = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
        "val_dataset_hf = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
        "test_dataset_hf = Dataset.from_pandas(test_df_final.reset_index(drop=True))"
      ],
      "metadata": {
        "id": "JkfDv9kKL3j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ METRICS\n",
        "acc_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric  = evaluate.load(\"f1\")\n",
        "prec_metric = evaluate.load(\"precision\")\n",
        "rec_metric  = evaluate.load(\"recall\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": acc_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "        # use macro F1 to match metric_for_best_model\n",
        "        \"f1_macro\": f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
        "        # keep weighted variants if you like viewing them\n",
        "        \"precision_weighted\": prec_metric.compute(predictions=preds, references=labels, average=\"weighted\")[\"precision\"],\n",
        "        \"recall_weighted\": rec_metric.compute(predictions=preds, references=labels, average=\"weighted\")[\"recall\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "GpOaKJ8wL5ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ MODEL FACTORY (RESIZES EMBEDDINGS TO MATCH TOKENIZER)\n",
        "def build_model(dropout: float | None = None):\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        MODEL_NAME_TWITTER,\n",
        "        num_labels=5,\n",
        "        problem_type=\"single_label_classification\",\n",
        "        id2label={0:\"Extremely Negative\",1:\"Negative\",2:\"Neutral\",3:\"Positive\",4:\"Extremely Positive\"},\n",
        "        label2id={\"Extremely Negative\":0,\"Negative\":1,\"Neutral\":2,\"Positive\":3,\"Extremely Positive\":4},\n",
        "        # If dropout is provided from Optuna, apply it to all relevant fields\n",
        "        hidden_dropout_prob=dropout if dropout is not None else None,\n",
        "        attention_probs_dropout_prob=dropout if dropout is not None else None,\n",
        "        classifier_dropout=dropout if dropout is not None else None,\n",
        "    )\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME_TWITTER, config=config)\n",
        "    # keep tokenizer/model in sync (because you added <url>/<user>/<number>)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    if tokenizer.pad_token_id is not None:\n",
        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
        "    return model"
      ],
      "metadata": {
        "id": "8Zv4ErzKIxAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ OPTUNA OBJECTIVE ‚Äî logs to Weights & Biases per trial\n",
        "def objective(trial):\n",
        "    import wandb\n",
        "\n",
        "    # ---- Hyperparams to search (your requested space)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 3e-5, log=True)\n",
        "    max_length    = trial.suggest_categorical(\"max_length\", [96, 128])\n",
        "    epochs        = trial.suggest_int(\"epochs\", 3, 4)\n",
        "    dropout       = trial.suggest_float(\"dropout\", 0.08, 0.18)\n",
        "    patience = trial.suggest_int(\"patience\", 2, 4)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32])\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", [0.0, 0.01, 0.02])\n",
        "   # warmup_ratio = trial.suggest_categorical(\"warmup_ratio\", [0.05, 0.1])\n",
        "\n",
        "    # ---- Start a fresh W&B run for this trial\n",
        "    run = wandb.init(\n",
        "        project=\"hf-electra\",\n",
        "        name=f\"trial-{trial.number}\",\n",
        "        reinit=True,\n",
        "        config={\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"max_length\": max_length,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"patience\": patience,\n",
        "            \"epochs\": epochs,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            #\"warmup_ratio\": warmup_ratio,\n",
        "            \"dropout\": dropout,\n",
        "            \"model_name\": MODEL_NAME_TWITTER,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # ---- Tokenize for this trial's max_length\n",
        "        def preprocess(examples):\n",
        "            enc = tokenizer(\n",
        "                examples[\"cleaned_tweets\"],\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=max_length,\n",
        "            )\n",
        "            enc[\"labels\"] = examples[\"label\"]\n",
        "            return enc\n",
        "\n",
        "        print(f\"[DEBUG] Trial #{trial.number} ‚Üí max_length = {max_length}\")\n",
        "\n",
        "\n",
        "        train_tok = train_dataset_hf.map(preprocess, batched=True, remove_columns=train_dataset_hf.column_names)\n",
        "        val_tok   = val_dataset_hf.map(preprocess,   batched=True, remove_columns=val_dataset_hf.column_names)\n",
        "        train_tok.set_format(type=\"torch\")\n",
        "        val_tok.set_format(type=\"torch\")\n",
        "\n",
        "        # ---- Build model (now receives dropout)\n",
        "        model = build_model(dropout=dropout)\n",
        "\n",
        "        # ---- Training args with W&B reporting\n",
        "        args = TrainingArguments(\n",
        "            output_dir=f\"./hf_roberta_optuna_FINAL/{trial.number}\",\n",
        "            run_name=f\"trial-{trial.number}\",        # W&B run name\n",
        "            report_to=[\"wandb\"],                     # enable W&B logging\n",
        "\n",
        "            learning_rate=learning_rate,\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            per_device_eval_batch_size=batch_size,\n",
        "            num_train_epochs=epochs,\n",
        "            weight_decay=weight_decay,\n",
        "\n",
        "            #FIX: correct argument name\n",
        "            eval_strategy=\"steps\",\n",
        "            save_strategy=\"steps\",\n",
        "            eval_steps=300,\n",
        "            save_steps=300,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"accuracy\",\n",
        "            greater_is_better=True,\n",
        "\n",
        "            save_total_limit=1,                      # keep disk usage tiny\n",
        "            logging_steps=100,\n",
        "            seed=42,\n",
        "            fp16=torch.cuda.is_available(),\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=args,\n",
        "            train_dataset=train_tok,\n",
        "            eval_dataset=val_tok,\n",
        "            tokenizer=tokenizer,\n",
        "            data_collator=default_data_collator,\n",
        "            compute_metrics=compute_metrics,\n",
        "            callbacks=[EarlyStoppingCallback(early_stopping_patience=patience)],\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        metrics = trainer.evaluate()\n",
        "\n",
        "        # record best checkpoint dir for this trial\n",
        "        best_ckpt = trainer.state.best_model_checkpoint\n",
        "        trial.set_user_attr(\"best_checkpoint\", best_ckpt)\n",
        "\n",
        "        # ensure best model (already loaded) is saved in output_dir\n",
        "        trainer.save_model()\n",
        "\n",
        "        # Log final eval metrics explicitly too\n",
        "        wandb.log({\n",
        "            \"final_eval/accuracy\": metrics.get(\"eval_accuracy\"),\n",
        "            \"final_eval/f1_macro\": metrics.get(\"eval_f1_macro\"),\n",
        "        })\n",
        "\n",
        "        # üîß Return the SAME metric used for model selection\n",
        "        return metrics[\"eval_accuracy\"]\n",
        "\n",
        "    finally:\n",
        "        # Ensure the run is closed even if an error occurs\n",
        "        wandb.finish()"
      ],
      "metadata": {
        "id": "6CYwNLEC_sZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ RUN OPTUNA\n",
        "study = optuna.create_study(direction=\"maximize\", study_name=\"hf-robertatwitter-attempt3\")\n",
        "study.optimize(objective, n_trials=10)\n",
        "print(\"Best value:\", study.best_value)\n",
        "print(\"Best params:\", study.best_params)"
      ],
      "metadata": {
        "id": "R1M7C8dxMEGP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from shutil import copytree\n",
        "from pathlib import Path\n",
        "\n",
        "best_trial = study.best_trial\n",
        "best_dir = best_trial.user_attrs.get(\"best_checkpoint\", None)\n",
        "assert best_dir is not None, \"No best_checkpoint found on the best trial.\"\n",
        "\n",
        "FINAL_DIR = Path(\"./best_model_autosaved\")\n",
        "copytree(best_dir, FINAL_DIR, dirs_exist_ok=True)\n",
        "print(\"‚úÖ Best model copied to:\", FINAL_DIR)"
      ],
      "metadata": {
        "id": "-PDCj4SQwOHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compression techniques"
      ],
      "metadata": {
        "id": "ane7E-1vKJKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import os, torch, copy\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch import nn\n",
        "import torch.nn.utils.prune as prune\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Point to your saved checkpoint folder (where config.json + pytorch_model.bin live)\n",
        "BASE_DIR = \"./best_model_autosaved\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(BASE_DIR).to(DEVICE)\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_DIR)"
      ],
      "metadata": {
        "id": "F0TIRPdAxBwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_on(df, model, tokenizer, max_length=192, batch_size=64, device=DEVICE, desc=\"Eval\"):\n",
        "    class TweetDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, dataframe, tokenizer, max_length=128):\n",
        "            self.texts = dataframe[\"cleaned_tweets\"].tolist()\n",
        "            self.labels = dataframe[\"label\"].tolist()\n",
        "            self.tokenizer = tokenizer\n",
        "            self.max_length = max_length\n",
        "        def __len__(self): return len(self.texts)\n",
        "        def __getitem__(self, idx):\n",
        "            enc = self.tokenizer(\n",
        "                str(self.texts[idx]),\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=self.max_length,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "            return {\n",
        "                \"input_ids\": enc[\"input_ids\"][0],\n",
        "                \"attention_mask\": enc[\"attention_mask\"][0],\n",
        "                \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
        "            }\n",
        "\n",
        "    ds = TweetDataset(test_df_final, tokenizer, max_length=max_length)\n",
        "    dl = DataLoader(ds, batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "    model = model.to(device).eval()\n",
        "    all_y, all_p = [], []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids = b[\"input_ids\"].to(device)\n",
        "            att = b[\"attention_mask\"].to(device)\n",
        "            y   = b[\"labels\"].to(device)\n",
        "            logits = model(ids, attention_mask=att).logits\n",
        "            p = logits.argmax(dim=1)\n",
        "            all_y.extend(y.cpu().numpy().tolist())\n",
        "            all_p.extend(p.cpu().numpy().tolist())\n",
        "\n",
        "    print(f\"{desc} accuracy:\", accuracy_score(all_y, all_p))\n",
        "    print(classification_report(all_y, all_p, digits=4))"
      ],
      "metadata": {
        "id": "ftlBHvrZmAnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compressed_models(base_model):\n",
        "    compressed = {}\n",
        "\n",
        "    # 1) Dynamic Quantization (CPU-only module)\n",
        "    cpu_model = copy.deepcopy(base_model).to(\"cpu\")\n",
        "    qmodel = torch.quantization.quantize_dynamic(\n",
        "        cpu_model,\n",
        "        {nn.Linear},\n",
        "        dtype=torch.qint8\n",
        "    )\n",
        "    qmodel = qmodel.eval()  # ‚úÖ Set to eval mode\n",
        "    compressed[\"quantized_cpu\"] = qmodel\n",
        "\n",
        "    # 2) Pruning (unstructured L1 across Linear layers)\n",
        "    pruned = copy.deepcopy(base_model).to(DEVICE)\n",
        "    params_to_prune = []\n",
        "    for m in pruned.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            params_to_prune.append((m, \"weight\"))\n",
        "\n",
        "    if len(params_to_prune) > 0:\n",
        "        prune.global_unstructured(\n",
        "            params_to_prune,\n",
        "            pruning_method=prune.L1Unstructured,\n",
        "            amount=0.40,  # 40% sparsity\n",
        "        )\n",
        "        # Remove reparametrization to bake in the zeroed-out weights\n",
        "        for (m, _) in params_to_prune:\n",
        "            try:\n",
        "                prune.remove(m, \"weight\")\n",
        "            except Exception:\n",
        "                pass\n",
        "    pruned = pruned.eval()  # ‚úÖ Set to eval mode\n",
        "    compressed[\"pruned\"] = pruned\n",
        "\n",
        "    # 3) FP16 (good for GPU inference)\n",
        "    half_model = copy.deepcopy(base_model).half().to(DEVICE)\n",
        "    half_model = half_model.eval()  # ‚úÖ Set to eval mode\n",
        "    compressed[\"fp16\"] = half_model\n",
        "\n",
        "    return compressed"
      ],
      "metadata": {
        "id": "mbMgGGcjwiY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Base FP32\n",
        "evaluate_on(test_df_final, model, tokenizer, max_length=192, desc=\"Base FP32\")\n",
        "\n",
        "# Get the compressed models dictionary\n",
        "compressed_models_dict = compressed_models(model)\n",
        "\n",
        "# FP16 (GPU)\n",
        "evaluate_on(test_df_final, compressed_models_dict[\"fp16\"], tokenizer, max_length=192, desc=\"FP16\")\n",
        "\n",
        "# Pruned (GPU)\n",
        "evaluate_on(test_df_final, compressed_models_dict[\"pruned\"], tokenizer, max_length=192, desc=\"Pruned\")\n",
        "\n",
        "# Quantized (CPU)\n",
        "evaluate_on(test_df_final, compressed_models_dict[\"quantized_cpu\"], tokenizer, max_length=192, device=\"cpu\", desc=\"Quantized CPU\")"
      ],
      "metadata": {
        "id": "nZrTqC97wk7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUT_DIR = \"/content/drive/MyDrive/◊™◊ï◊ê◊® ◊©◊†◊ô/deep\" #write the output directory here\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# A) Save FP16\n",
        "fp16_dir = os.path.join(OUT_DIR, \"fp16\")\n",
        "os.makedirs(fp16_dir, exist_ok=True)\n",
        "compressed_models_dict[\"fp16\"].save_pretrained(fp16_dir)\n",
        "tokenizer.save_pretrained(fp16_dir)\n",
        "\n",
        "# B) Save pruned (after prune.remove, save_pretrained works)\n",
        "pruned_dir = os.path.join(OUT_DIR, \"pruned\")\n",
        "os.makedirs(pruned_dir, exist_ok=True)\n",
        "compressed_models_dict[\"pruned\"].save_pretrained(pruned_dir)\n",
        "tokenizer.save_pretrained(pruned_dir)\n",
        "\n",
        "# C) Save quantized CPU (state_dict + a tiny loader script)\n",
        "q_dir = os.path.join(OUT_DIR, \"quantized_cpu\")\n",
        "os.makedirs(q_dir, exist_ok=True)\n",
        "torch.save(compressed_models_dict[\"quantized_cpu\"].state_dict(), os.path.join(q_dir, \"quantized_state_dict.pt\"))\n",
        "\n",
        "# Save a small loader so future-you can reload easily\n",
        "with open(os.path.join(q_dir, \"load_quantized.py\"), \"w\") as f:\n",
        "    f.write(\n",
        "        \"import torch\\n\"\n",
        "        \"from torch import nn\\n\"\n",
        "        \"from transformers import AutoModelForSequenceClassification\\n\"\n",
        "        \"def load_quantized(model_dir, state_path):\\n\"\n",
        "        \"    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\\n\"\n",
        "        \"    model = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\\n\"\n",
        "        \"    sd = torch.load(state_path, map_location='cpu')\\n\"\n",
        "        \"    model.load_state_dict(sd, strict=False)\\n\"\n",
        "        \"    model.eval()\\n\"\n",
        "        \"    return model\\n\"\n",
        "    )\n",
        "\n",
        "print(\"Saved to:\", OUT_DIR)"
      ],
      "metadata": {
        "id": "M4Eu3BWIwny6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dWAttE_FxYRJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}