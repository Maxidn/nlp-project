# Tweet Sentiment Classification with RoBERTa and ELECTRA

This project fine-tunes two transformer-based models â€” RoBERTa and ELECTRA â€” on a COVID-19 tweet sentiment classification task. It also applies compression techniques (quantization, pruning, FP16) to reduce model size and improve inference efficiency.

## ðŸ§  Task
Multi-class classification of tweet sentiments into five categories:
- Extremely Negative
- Negative
- Neutral
- Positive
- Extremely Positive

Dataset: [Corona NLP COVID-19 Tweets Dataset (Kaggle)](https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification)

## ðŸ—‚ï¸ Project Structure

project-root/
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ roberta_full_code.ipynb # Fine-tuning + compression using full PyTorch code
â”‚ â”œâ”€â”€ roberta_hf_trainer.ipynb # Fine-tuning + compression using HuggingFace Trainer API
â”‚ â”œâ”€â”€ electra_full_code.ipynb # (to be added)
â”‚ â”œâ”€â”€ electra_hf_trainer.ipynb # (to be added)
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ train.csv # Cleaned training set
â”‚ â”œâ”€â”€ test.csv # Cleaned test set
â”‚
â”œâ”€â”€ checkpoints/
â”‚ â””â”€â”€ roberta_full/
â”‚ â”œâ”€â”€ fp32/ # Full-precision model
â”‚ â”œâ”€â”€ fp16/ # Mixed precision (for GPU inference)
â”‚ â”œâ”€â”€ pruned/ # Unstructured pruning
â”‚ â””â”€â”€ quantized/ # Quantized (for CPU inference)
â”‚
â”œâ”€â”€ paper/
â”‚ â””â”€â”€ final_paper.pdf
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt # (optional)


## ðŸ§ª Models and Techniques

- **RoBERTa (cardiffnlp/twitter-roberta-base)** fine-tuned using:
  - Full PyTorch training loop (manual scheduler, early stopping, Optuna tuning)
  - HuggingFace Trainer API

- **Compression methods** applied to best model:
  - **Pruning**: 40% unstructured L1
  - **Quantization**: dynamic quantization with `torch.quantization`
  - **FP16**: model half-precision

## ðŸ“Š Evaluation
The model is evaluated using:
- Accuracy
- Precision / Recall / F1 (macro + weighted)
- Per-class performance reports

## ðŸ“ How to Run

> ðŸ’¡ **Recommended**: Open notebooks in Google Colab

1. Replace the W&B API key (`wandb.login(...)`) with your own  
2. Mount your Google Drive (`drive.mount(...)`) where:
   - Datasets are located (`Corona_NLP_train_clean.csv`, `Corona_NLP_test_clean.csv`)
   - Trained models and compression results are saved

3. Run the cells as-is

## ðŸ’¾ Files Needed in Google Drive

- `Corona_NLP_train_clean.csv`, `Corona_NLP_test_clean.csv`
- Folder with best fine-tuned RoBERTa model
- Folder with compressed models:
  - `/fp16/`, `/pruned/`, `/quantized/` with saved models
  - `load_quantized.py` is included to help re-load quantized models

## ðŸ“Ž Notes

- Outputs are visible in notebooks for review
- `wandb` tracking logs hyperparameters, metrics, and trials
- Optuna is used for tuning learning rate, dropout, batch size, etc.
- The same architecture is applied to ELECTRA (coming soon)

## ðŸ‘¥ Authors
- Maximiliano Niemetz
- Chen Ben Halfi

---
For questions or issues, please open an issue or contact the authors.
